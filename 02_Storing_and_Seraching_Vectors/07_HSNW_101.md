# HNSW (Hierarchical Navigable Small World) — Notes

## 0. What HNSW Fixes from NSW

**NSW problems:**
- Bad entry point → wrong region
- Single layer → only local crawling
- Local minima → can't "teleport" out

**HNSW solution:**
> Multiple graphs stacked: top = sparse (long jumps), bottom = dense (precision)

---

## 1. Structure: Stack of Layers

**Analogy:** Maps of same city
- Top layer: major highways between hubs
- Bottom layer: all streets and alleyways

**Properties:**
- Every node in layer 0 (bottom)
- Only some in layer 1
- Fewer in layer 2
- Even fewer in layer 3+

**Result:** Higher layers smaller and sparser

---

## 2. Layer Assignment

### How Many Layers?
- Not fixed ahead of time
- Emerges as `max_level_in_graph = max(L(node))`
- Grows roughly as log(N)

### How Node Gets Level L?

**Random (like skip list):**
```text
L = floor(-log(U) * mL)
where U ∈ (0,1] uniform random
```

**Distribution:**
- Most nodes: L = 0
- Some: L = 1
- Fewer: L = 2
- Very few: L = 3+

**Why random > heuristic:**
- Predictable layer sizes
- Good navigability
- Easy incremental insertion

---

## 3. Layer Traversal

**No cross-layer edges**

Each layer = own graph:
- Layer 3 edges connect layer 3 nodes only
- Layer 2 edges connect layer 2 nodes only

**Descending:**
- Finish search at node `ep` in layer l
- Same node exists in all lower layers
- Reuse as entry point in layer l-1
- "Zoom in" but stay at same location

---

## 4. Core Algorithms

### 4A. Bounded Search (The Engine)

Used for:
- Query-time at layer 0 (with efSearch)
- Insert-time neighbor discovery (with efConstruction)
```python
function SEARCH_LAYER(q, ep, ef, layer):
    visited = set()
    
    # candidates: min-heap by distance (closest first)
    candidates = MinHeap()
    # best: max-heap by distance (worst among best on top)
    best = MaxHeap()
    
    d0 = dist(q, ep)
    candidates.push((d0, ep))
    best.push((d0, ep))
    visited.add(ep)

    while candidates not empty:
        (dc, c) = candidates.pop_min()     # closest unexplored
        (dw, w) = best.peek_max()          # worst in best
        
        # early stop
        if dc > dw:
            break
        
        for n in neighbors(c, layer):
            if n in visited:
                continue
            visited.add(n)
            
            dn = dist(q, n)
            (dw, w) = best.peek_max()
            
            if best.size < ef or dn < dw:
                candidates.push((dn, n))
                best.push((dn, n))
                
                if best.size > ef:
                    best.pop_max()

    return best.items()   # ef closest nodes
```

**Key:**
- `candidates` drives exploration
- `best` maintains best ef nodes
- Early stop keeps it fast

---

### 4B. Query-Time Search
```python
function HNSW_QUERY(q, k, efSearch):
    ep = enterpoint
    
    # 1) Fast greedy descent from top to layer 1
    for layer from maxLayer down to 1:
        changed = true
        while changed:
            changed = false
            for n in neighbors(ep, layer):
                if dist(q, n) < dist(q, ep):
                    ep = n
                    changed = true

    # 2) Thorough bounded search at layer 0
    best0 = SEARCH_LAYER(q, ep, efSearch, layer=0)

    # 3) Return top-k
    return top_k_closest(best0, k)
```

**Strategy:**
- Upper layers: cheap greedy (single-path, sparse)
- Layer 0: bounded search (quality)

---

### 4C. Insert/Build
```python
function HNSW_INSERT(x):
    Lx = RANDOM_LEVEL()
    
    if graph is empty:
        enterpoint = x
        maxLayer = Lx
        add x to layers 0..Lx with no edges
        return

    ep = enterpoint

    # 1) Greedy descent to layer Lx+1
    for layer from maxLayer down to (Lx + 1):
        changed = true
        while changed:
            changed = false
            for n in neighbors(ep, layer):
                if dist(x, n) < dist(x, ep):
                    ep = n
                    changed = true

    # 2) Connect x in its layers
    for layer from min(Lx, maxLayer) down to 0:
        
        # 2a) Find candidates
        candidates = SEARCH_LAYER(x, ep, efConstruction, layer)
        
        # 2b) Select neighbors (with diversification)
        neighbors_x = SELECT_NEIGHBORS(candidates, M, layer)
        
        # 2c) Add bidirectional edges
        for y in neighbors_x:
            add_edge(x, y, layer)
            add_edge(y, x, layer)
            
            # 2d) Prune if exceeds degree
            if degree(y, layer) > M:
                prune_neighbors(y, M, layer)
        
        # 2e) Set ep for next layer
        ep = closest_node_to_x(candidates)

    # 3) Update global entry if needed
    if Lx > maxLayer:
        enterpoint = x
        maxLayer = Lx
```

---

## 5. Three Critical Parameters

### (1) M (graph degree)
- Neighbors per node per layer
- Higher M: better connectivity, better recall, more memory, slower search
- **Typical:** 16-48

### (2) efConstruction (build effort)
- Candidates considered when inserting
- Higher: better graph quality, slower build
- **Critical for recall**

### (3) efSearch (query effort)
- Candidates kept while searching
- Higher: more nodes visited, higher recall, slower
- **Can tune without rebuilding** ✓

---

## 6. Diversification (Why Closest-M Fails)

### The Trap
Picking M closest neighbors creates:
- Cliques (all neighbors nearly identical)
- Same tiny region
- Poor long-range navigation
- Graph loses coverage

**Result:** Greedy search gets trapped locally

---

### HNSW Diversification Rule

**Goal:** Keep neighbors close to X, but far from each other
```python
neighbors = empty list
sort C by dist(X, c) ascending

for c in C:
    good = true
    for n in neighbors:
        if dist(c, n) < dist(c, X):
            good = false
            break
    if good:
        neighbors.add(c)
    if size(neighbors) == M:
        break

return neighbors
```

**Rule:** Reject candidate c if:
> Exists neighbor n where n is closer to c than c is to X

**Interpretation:**
> "If c reachable via another neighbor, don't keep it"

**Result:**
- Some very close neighbors
- Some farther but directionally different
- Short paths locally + escape routes globally

---

## 7. Pruning (Enforcing Degree Limits)

### When It Happens
Insert: X connects to Y → Y has M+1 neighbors → must prune

### How It Works
**Not random removal** — reapply diversification rule
```python
function PRUNE(y, M, layer):
    candidates = neighbors(y, layer)
    neighbors_y = SELECT_NEIGHBORS(candidates, M, center=y)
    replace neighbors(y, layer) with neighbors_y
```

**Result:**
- Old neighbors may drop
- New node X may not survive
- Local graph re-balances
- **Graph is self-healing**

---

## 8. Deletes & Tombstones

### Why Deletes Are Hard
- Node may be bridge between regions
- Removal disconnects graph
- Need to rewire neighbors globally
- Expensive rebuilds

**Solution:** Don't do hard deletes

---

### Tombstones (Practical Solution)

**Definition:** Mark node as dead, don't remove

**Behavior:**
- Node remains in graph
- Skipped in search results
- Still traversed for navigation

**Why it works:**
- Position in space still valid
- Still points to nearby regions
- Removing hurts navigation more

**During search:**
- Don't return tombstoned nodes
- Continue exploring their neighbors
- Preserves recall, connectivity, latency

---

### When Tombstones Become Problem
Too many tombstones:
- Search becomes noisy
- Memory waste
- Recall degrades

**Solution:** Periodic full rebuild or background compaction

---

## 9. Entry Points: One vs Many

### What Is Entry Point?
Node where search starts at top layer

### Why HNSW Uses ONE Entry Point

**Naive approach:** Start from multiple random nodes
- ✓ Higher recall
- ✗ Latency multiplies
- ✗ Redundant exploration

**HNSW insight:**
> Hierarchy replaces multiple starts

**Why top layer = perfect entry:**
- Very few nodes
- Widely spread
- Sparse routing backbone
- Greedy navigation quickly reaches right region

**One smart entry on coarse representation > multiple random starts**

---

### Why Single Global Entry Point?

HNSW maintains:
- One `enterpoint`
- Always the node with highest level

**Benefits:**
- Exists in all top layers
- Guaranteed reachable
- Stabilizes structure
- Simplifies insert/query logic

---

### When Multiple Entry Points Make Sense
- Graph quality poor
- efSearch very small
- Data extremely multi-modal
- Sharded across machines

**But:** Most systems still use one per shard

---

## 10. Why Hierarchy Works

**HNSW simulates "many starting points" via:**
- One start + coarse-to-fine descent

**At top layer:**
- Each step = large distance in embedding space
- Global routing

**At lower layers:**
- Movement smaller
- Precision increases

**Result:**
- High recall
- Low latency
- Predictable performance

---

## Key Takeaways

1. **Hierarchy replaces** multiple entry points
2. **Diversification prevents** cliques
3. **Pruning maintains** degree limits while preserving connectivity
4. **Tombstones preserve** graph structure during deletes
5. **One entry + layers** = fast, reliable navigation
6. **efSearch tunable** without rebuild (production superpower)

---

## Production Tradeoffs

**HNSW gives:**
- Excellent recall vs latency
- Online inserts
- Good in 256-3072 dims
- Works with cosine, L2, dot

**HNSW costs:**
- High memory (graph edges)
- Heavy build time
- Eventually needs rebuild (tombstones)

---

**Complete.** You now understand HNSW end-to-end.
